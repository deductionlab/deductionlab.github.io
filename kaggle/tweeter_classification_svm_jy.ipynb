{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text\n\nfrom tqdm import tqdm\nimport re\n\n#import for another version using NLTK\nimport datetime\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer# from sklearn.xvm import SVC\nfrom nltk.tokenize import TweetTokenizer\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading data \ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sampling over data train\ntrain_arrays, test_arrays, train_labels, test_labels = train_test_split(X_train,y_train,test_size=0.05)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"code","source":"# Normalize(Remove stop words, stemmatize) tweets\n\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef normalizer(tweet):\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    tokens = word_tokenize(only_letters)[2:]\n    lower_case = [l.lower() for l in tokens]\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n    return lemmas\n\ntrain['normalize_text'] = train['text'].apply(normalizer)\n\n#train.head()\nprint (train.target.value_counts())\n\n# svc  = SVC()\n# svc.fit(X_train, Y_train)\n# predictions = svc.predict(X_test)\n\n# print(classification_report(y_test, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use bi-gram/ tri-gram\nfrom nltk import ngrams\ndef ngrams(input_list):\n    #onegrams = input_list\n    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n    return bigrams+trigrams\n\nnormalize_text_aggr_arr = []\nfor row in train['normalize_text']:\n    aggr_str = \" \".join(row)\n    normalize_text_aggr_arr.append(aggr_str) \n\n#ngram_normalize_text_aggr_arr = []\n#for row in train['ngram_normalize_text']:\n#    aggr_str = \" \".join(row)\n#    ngram_normalize_text_aggr_arr.append(aggr_str) \n\ntrain['normalize_text_aggr'] = normalize_text_aggr_arr\n#train['ngram_normalize_text_aggr'] = normalize_text_aggr_arr\n#train[[\"text\",\"normalized_text\",\"normalize_text_aggr\", \"ngram_normalize_text_aggr\"]]\n# train['ngram_normalize_text'] = train['normalize_text'].apply(ngrams)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use word tokenizer \ntrain['word_tokenized_text'] = train['text'].apply(word_tokenize)\n\n#Use tweet tokenizer\ntt = TweetTokenizer()\ntrain['tweet_tokenized_text'] = train['text'].apply(tt.tokenize)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\ndef count_words(input):\n    cnt = collections.Counter()\n    for row in input:\n        for word in row:\n            cnt[word] += 1\n    return cnt\n\n#Most frequent words for disaster tweets\ndict1 = train[train.target == 1][['ngram_normalize_text']].apply(count_words).ngram_normalize_text\nprint (dict1.most_common(30))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequent words for disaster tweets\ndict1 = train[train.target == 1][['normalize_text']].apply(count_words).normalize_text\nprint (dict1.most_common(30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequent words for NON-disaster tweets\ndict1 = train[train.target == 0][['ngram_normalize_text']].apply(count_words).ngram_normalize_text\nprint (dict1.most_common(30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequent words for disaster tweets\ndict1 = train[train.target == 1][['word_tokenized_text']].apply(count_words).word_tokenized_text\nprint (dict1.most_common(30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequent words for disaster tweets\ndict1 = train[train.target == 1][['tweet_tokenized_text']].apply(count_words).tweet_tokenized_text\nprint (dict1.most_common(30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Vectorization - normalized\nvectorized_data = count_vectorizer.fit_transform(train.normalize_text_aggr)\nindexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))\ntargets = train['target']\nprint (indexed_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Vectorization\n# vectorized_data = count_vectorizer.fit_transform(train.text)\n# indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))\n# targets = train['target']\n#print (indexed_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split train and validation data\ndata_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\ndata_train_index = data_train[:,0]\ndata_train = data_train[:,1:]\ndata_test_index = data_test[:,0]\ndata_test = data_test[:,1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting svm classfier\nfrom sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nsvm_clf =svm.SVC(kernel = 'linear')\n\nsvm_clf_output = svm_clf.fit(data_train, targets_train)\nsvm_clf.score(data_train, targets_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_clf.score(data_test, targets_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Making submission\ntest_pred = svm_clf.predict(X_test)\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}